<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lucas-Kanade Distraction Tracker (Web)</title>
    <!-- Load Tailwind CSS for modern, responsive styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Define Inter font and ensure the body fills the screen */
        body {
            font-family: 'Inter', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #0d1117; /* Dark background */
        }
        .container {
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 20px;
        }
        #video, #canvasOutput {
            max-width: 100%;
            height: auto;
            border-radius: 12px;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.5);
        }
        #canvasOutput {
            /* Canvas will take the place of the hidden video element */
            background-color: #161b22;
        }
        .info-card {
            background-color: #161b22;
            padding: 1rem 2rem;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
        }
    </style>
    <!-- Load the official OpenCV.js library. We use async + onload for strict sequential execution. -->
    <script async src="https://docs.opencv.org/4.x/opencv.js" onload="onOpenCVLoaded()"></script>
</head>
<body>

<div class="container min-h-screen text-white">
    <h1 class="text-3xl font-bold mb-6 text-green-400">Lucas-Kanade Distraction Tracker (Web)</h1>
    <p class="mb-4 text-center max-w-lg text-gray-400">
        This is the web-deployed version. All face detection, tracking, and Distraction counting 
        happen in your browser using **OpenCV.js**.
    </p>

    <div id="status" class="info-card mb-6 text-sm text-gray-300">Loading OpenCV.js...</div>

    <div class="relative w-full max-w-2xl">
        <!-- The video element captures the webcam stream (can be hidden) -->
        <video id="video" hidden autoplay playsinline></video>
        <!-- The canvas element is where the processed frames are drawn -->
        <canvas id="canvasOutput" class="w-full"></canvas>
        
        <!-- Overlay for displaying Distraction count -->
        <div class="absolute top-4 left-4 info-card">
            <span class="text-lg font-mono">Distractions: <span id="Distraction-count" class="text-green-400 font-extrabold">0</span></span>
        </div>
    </div>

    <div class="mt-6 info-card">
        <p class="text-sm text-gray-400">Adjust the tracking parameters for your environment for better accuracy.</p>
    </div>
</div>

<script>
    // Flag to ensure the custom logic runs only once after OpenCV is ready.
    let isMainLoopInitialized = false;

    // The function executed when the external script file has completely loaded.
    function onOpenCVLoaded() {
        // OpenCV.js uses a separate callback (onRuntimeInitialized) to indicate its internal C++ environment is ready.
        // We must combine the script load (onload) and the internal runtime init.
        cv.onRuntimeInitialized = () => {
            if (!isMainLoopInitialized) {
                isMainLoopInitialized = true;
                main();
            }
        };
    }

    // --- Main Execution Function (The guaranteed safe entry point) ---
    async function main() {

        // --- DOM Elements and Context ---
        const video = document.getElementById('video');
        const canvasOutput = document.getElementById('canvasOutput');
        const statusElement = document.getElementById('status');
        const DistractionCountElement = document.getElementById('Distraction-count');
        const context = canvasOutput.getContext('2d');

        // --- Configuration Constants ---
        const FRAME_WIDTH = 640;
        const FRAME_HEIGHT = 480;

        // Distraction Detection Constants (Based on Python tuning)
        const Distraction_FRAME_THRESHOLD = 2; 
        const MAX_Distraction_FRAMES = 100;    

        // --- Global State Variables (Mats initialized inside the safe zone) ---
        let faceCascade = new cv.CascadeClassifier();
        let eyeCascade = new cv.CascadeClassifier();
        let old_gray = new cv.Mat();
        let old_points = new cv.Mat();

        let trackingActive = false;
        let DistractionCounter = 0;
        let DistractionClosedFrames = 0;
        
        let isVideoReady = false;
        let areCascadesReady = false; 

        // --- Cascade Download and Initialization ---
        async function downloadFile(url, destFileName) {
            const response = await fetch(url);
            const buffer = await response.arrayBuffer();
            // cv is guaranteed to be defined here
            cv.FS_createDataFile('/', destFileName, new Uint8Array(buffer), true, false, false);
            return destFileName;
        }

        async function initializeCascades() {
            try {
                statusElement.textContent = "Camera Active. Downloading Haar Cascades...";
                
                const facePath = await downloadFile('https://raw.githubusercontent.com/opencv/opencv/4.x/data/haarcascades/haarcascade_frontalface_default.xml', 'haarcascade_face.xml');
                const eyePath = await downloadFile('https://raw.githubusercontent.com/opencv/opencv/4.x/data/haarcascades/haarcascade_eye.xml', 'haarcascade_eye.xml');
                
                faceCascade.load(facePath);
                eyeCascade.load(eyePath);

                if (faceCascade.empty() || eyeCascade.empty()) {
                    throw new Error("Failed to load cascade classifiers.");
                }
                
                areCascadesReady = true; 
                statusElement.textContent = "Cascades loaded. Auto-detecting eyes...";
            } catch (e) {
                statusElement.textContent = `FATAL ERROR: Failed to load cascades. ${e.message}`;
                console.error(e);
                throw e;
            }
        }

        // --- Camera Initialization ---
        function startCamera() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                statusElement.textContent = "Error: Webcam access not supported by this browser.";
                return;
            }
            
            // Immediate camera permission request
            navigator.mediaDevices.getUserMedia({
                video: {
                    width: { ideal: FRAME_WIDTH },
                    height: { ideal: FRAME_HEIGHT }
                }
            })
                .then(stream => {
                    video.srcObject = stream;
                    video.onloadedmetadata = () => {
                        video.play();
                        canvasOutput.width = FRAME_WIDTH;
                        canvasOutput.height = FRAME_HEIGHT;
                        isVideoReady = true;
                        
                        // Start the continuous processing loop
                        window.requestAnimationFrame(processFrame);
                    };
                })
                .catch(err => {
                    statusElement.textContent = `Error accessing webcam: ${err.name} - ${err.message}. Permission was denied or camera is in use.`;
                    console.error('Error accessing webcam:', err);
                });
        }

        // --- Core CV: Eye Detection ---
        function detectAndInitializeEyes(frameGray, frameColor) {
            const faces = new cv.RectVector();
            const eyes = new cv.RectVector();
            const minSize = new cv.Size(100, 100);

            faceCascade.detectMultiScale(frameGray, faces, 1.3, 7, 0, minSize, new cv.Size(0, 0));

            if (faces.size() === 0) {
                faces.delete(); eyes.delete();
                return { success: false, points: null };
            }

            const faceRect = faces.get(0);
            
            // Draw Face Bounding Box
            cv.rectangle(frameColor, new cv.Point(faceRect.x, faceRect.y), 
                                     new cv.Point(faceRect.x + faceRect.width, faceRect.y + faceRect.height), 
                                     [255, 165, 0, 255], 2);

            const roiRect = new cv.Rect(faceRect.x, faceRect.y, faceRect.width, faceRect.height / 2);
            const roiGray = frameGray.roi(roiRect);

            eyeCascade.detectMultiScale(roiGray, eyes, 1.1, 8, 0, new cv.Size(20, 20), new cv.Size(0, 0));

            let initialPoints = null;

            if (eyes.size() === 2) {
                const pointsArray = [];
                for (let i = 0; i < eyes.size(); ++i) {
                    const eyeRect = eyes.get(i);
                    const center_x = faceRect.x + eyeRect.x + eyeRect.width / 2;
                    const center_y = faceRect.y + eyeRect.y + eyeRect.height / 2;
                    pointsArray.push(center_x, center_y);

                    // Draw Eye Bounding Box (optional, debugging visualization)
                    cv.rectangle(frameColor, new cv.Point(faceRect.x + eyeRect.x, faceRect.y + eyeRect.y), 
                                             new cv.Point(faceRect.x + eyeRect.x + eyeRect.width, faceRect.y + eyeRect.height), 
                                             [0, 255, 0, 255], 1);
                }
                
                initialPoints = cv.matFromArray(pointsArray.length / 2, 1, cv.CV_32FC2, pointsArray);
            }

            roiGray.delete(); faces.delete(); eyes.delete();
            
            return { success: initialPoints !== null, points: initialPoints }; 
        }

        // --- Main Processing Loop ---
        function processFrame() {
            if (!isVideoReady || !areCascadesReady) {
                window.requestAnimationFrame(processFrame);
                return;
            }

            // Create temporary Mats for the current frame processing
            const frame = new cv.Mat(FRAME_HEIGHT, FRAME_WIDTH, cv.CV_8UC4);
            const frameGray = new cv.Mat(FRAME_HEIGHT, FRAME_WIDTH, cv.CV_8UC1);

            context.drawImage(video, 0, 0, FRAME_WIDTH, FRAME_HEIGHT);
            let src = cv.imread(canvasOutput); // Read frame from canvas
            
            cv.cvtColor(src, frameGray, cv.COLOR_RGBA2GRAY);

            if (!trackingActive || old_points.rows < 2) {
                // --- Re-detection Phase ---
                
                const { success, points } = detectAndInitializeEyes(frameGray, src);

                if (success) {
                    // Tracking restored/initialized
                    old_points.delete(); 
                    old_points = points.clone(); 
                    points.delete();
                    
                    old_gray.delete();
                    old_gray = frameGray.clone(); 
                    
                    // Distraction COUNTING LOGIC (Using the large 100 frame window for robustness)
                    if (!trackingActive) {
                        if (DistractionClosedFrames >= Distraction_FRAME_THRESHOLD && DistractionClosedFrames <= MAX_Distraction_FRAMES) {
                            DistractionCounter++;
                            // console.log(`Distraction detected! Total Distractions: ${DistractionCounter} (Closed Duration: ${DistractionClosedFrames} frames)`);
                        } else if (DistractionClosedFrames > 0) {
                            // console.log(`Interruption detected (${DistractionClosedFrames} frames) but NOT counted as Distraction. Reason: ${DistractionClosedFrames < Distraction_FRAME_THRESHOLD ? 'Too short' : 'Too long'}.`);
                        }
                    }

                    trackingActive = true;
                    DistractionClosedFrames = 0;
                    
                } else {
                    // Detection failed (eyes closed/moved). Accumulate closed frames.
                    DistractionClosedFrames++;
                }

            } else {
                // --- Active Tracking Phase (Lucas-Kanade) ---

                const newPoints = new cv.Mat();
                const status = new cv.Mat();
                const err = new cv.Mat();
                
                const winSize = new cv.Size(21, 21);
                const criteria = new cv.TermCriteria(cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_COUNT, 20, 0.01);

                cv.calcOpticalFlowPyrLK(old_gray, frameGray, old_points, newPoints, status, err, winSize, 3, criteria);

                let goodPointsCount = 0;
                const goodNewPointsData = [];

                for (let i = 0; i < status.rows; ++i) {
                    if (status.data[i] === 1) {
                        const x = newPoints.data32F[i * 2];
                        const y = newPoints.data32F[i * 2 + 1];
                        goodPointsCount++;
                        goodNewPointsData.push(x, y);

                        // Draw the tracking circle
                        cv.circle(src, new cv.Point(Math.round(x), Math.round(y)), 7, [0, 255, 0, 255], -1, cv.LINE_AA);
                    }
                }
                
                if (goodPointsCount === 2) {
                    // Tracking successful
                    old_points.delete(); 
                    old_points = cv.matFromArray(goodNewPointsData.length / 2, 1, cv.CV_32FC2, goodNewPointsData);
                    
                    old_gray.delete();
                    old_gray = frameGray.clone();

                    DistractionClosedFrames = 0;

                } else {
                    // Tracking lost (Distraction or rapid movement)
                    // console.log("TRACKING LOST: Less than 2 points survived. Entering potential Distraction/re-detection phase.");
                    trackingActive = false;
                }

                // Cleanup Mats used in LK
                newPoints.delete();
                status.delete();
                err.delete();
            }

            // Display status text
            const statusMessage = trackingActive ? "STATUS: Tracking Eyes (Green Circles)" : "STATUS: Auto-detecting eyes...";
            cv.putText(src, statusMessage, new cv.Point(FRAME_WIDTH / 2 - 150, FRAME_HEIGHT - 20), 
                       cv.FONT_HERSHEY_SIMPLEX, 0.7, [255, 255, 255, 255], 2, cv.LINE_AA);

            // --- Final Display and Cleanup ---
            cv.imshow('canvasOutput', src);
            DistractionCountElement.textContent = DistractionCounter;
            statusElement.textContent = statusMessage.replace("STATUS: ", "");

            src.delete();
            frame.delete();
            frameGray.delete();

            // Loop: Request next frame
            window.requestAnimationFrame(processFrame);
        }

        // --- Execution Start (Guaranteed to run after OpenCV is fully ready) ---
        try {
            // 1. Start the camera immediately (requests permission pop-up)
            startCamera();

            // 2. Start downloading cascades in the background while video loads
            await initializeCascades();

        } catch (e) {
            console.error("Critical Initialization Failed:", e);
        }
    }
    
</script>
</body>
</html>
