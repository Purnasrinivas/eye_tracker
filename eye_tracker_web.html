<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web-Based Eye Tracker (OpenCV.js)</title>
    <!-- Load Tailwind CSS for modern, responsive styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Define Inter font and ensure the body fills the screen */
        body {
            font-family: 'Inter', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #0d1117; /* Dark background */
        }
        .container {
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 20px;
        }
        #video, #canvasOutput {
            max-width: 100%;
            height: auto;
            border-radius: 12px;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.5);
        }
        #canvasOutput {
            /* Canvas will take the place of the hidden video element */
            background-color: #161b22;
        }
        .info-card {
            background-color: #161b22;
            padding: 1rem 2rem;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
        }
    </style>
    <!-- Load the official OpenCV.js library -->
    <script async src="https://docs.opencv.org/4.x/opencv.js"></script>
</head>
<body>

<div class="container min-h-screen text-white">
    <h1 class="text-3xl font-bold mb-6 text-green-400">Lucas-Kanade Blink Tracker (Web)</h1>
    <p class="mb-4 text-center max-w-lg text-gray-400">
        This is the web-deployed version. All face detection, tracking, and blink counting 
        happen in your browser using **OpenCV.js**.
    </p>

    <div id="status" class="info-card mb-6 text-sm text-gray-300">Loading OpenCV.js...</div>

    <div class="relative w-full max-w-2xl">
        <!-- The video element captures the webcam stream (can be hidden) -->
        <video id="video" hidden autoplay playsinline></video>
        <!-- The canvas element is where the processed frames are drawn -->
        <canvas id="canvasOutput" class="w-full"></canvas>
        
        <!-- Overlay for displaying blink count -->
        <div class="absolute top-4 left-4 info-card">
            <span class="text-lg font-mono">Blinks: <span id="blink-count" class="text-green-400 font-extrabold">0</span></span>
        </div>
    </div>

    <div class="mt-6 info-card">
        <p class="text-sm text-gray-400">Adjust the tracking parameters for your environment for better accuracy.</p>
    </div>
</div>

<script>
    const video = document.getElementById('video');
    const canvasOutput = document.getElementById('canvasOutput');
    const statusElement = document.getElementById('status');
    const blinkCountElement = document.getElementById('blink-count');
    const context = canvasOutput.getContext('2d');

    // --- Configuration Constants (Matching Python version) ---
    const FRAME_WIDTH = 640;
    const FRAME_HEIGHT = 480;

    // Blink Detection Constant
    const BLINK_FRAME_THRESHOLD = 2; 
    const MAX_BLINK_FRAMES = 100; 

    // Global variables for OpenCV.js Mats and tracking state
    let faceCascade, eyeCascade;
    
    // IMPORTANT: Declare Mats globally, but do not initialize them yet.
    let old_gray; 
    let old_points; 

    let trackingActive = false;
    let blinkCounter = 0;
    let blinkClosedFrames = 0;
    
    // Readiness flags to control the flow
    let isVideoReady = false;
    let areCascadesReady = false; 

    // --- Haar Cascade Download and Initialization ---
    async function downloadFile(url, destFileName) {
        const response = await fetch(url);
        const buffer = await response.arrayBuffer();
        
        // FIX: Add a defensive check for cv here. If the code reaches here and cv is undefined, 
        // it must mean the promise chain or async loading is faulty. Throw an error instead.
        if (typeof cv === 'undefined') {
            throw new Error("OpenCV.js object 'cv' is not available during file creation.");
        }
        
        cv.FS_createDataFile('/', destFileName, new Uint8Array(buffer), true, false, false);
        return destFileName;
    }

    async function initializeCascades() {
        try {
            statusElement.textContent = "Camera Active. Downloading Haar Cascades...";
            
            // Download cascade files
            const facePath = await downloadFile('https://raw.githubusercontent.com/opencv/opencv/4.x/data/haarcascades/haarcascade_frontalface_default.xml', 'haarcascade_face.xml');
            const eyePath = await downloadFile('https://raw.githubusercontent.com/opencv/opencv/4.x/data/haarcascades/haarcascade_eye.xml', 'haarcascade_eye.xml');
            
            // Defensive checks before creating objects
            if (typeof cv === 'undefined') {
                 throw new Error("OpenCV.js object 'cv' is not available during cascade initialization.");
            }

            faceCascade = new cv.CascadeClassifier();
            eyeCascade = new cv.CascadeClassifier();
            
            faceCascade.load(facePath);
            eyeCascade.load(eyePath);

            if (faceCascade.empty() || eyeCascade.empty()) {
                throw new Error("Failed to load cascade classifiers.");
            }
            
            areCascadesReady = true; 
            statusElement.textContent = "Cascades loaded. Auto-detecting eyes...";

        } catch (e) {
            statusElement.textContent = `FATAL ERROR: Failed to load cascades. ${e.message}`;
            console.error(e);
        }
    }

    // --- Camera Initialization ---
    function startCamera() {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            statusElement.textContent = "Error: Webcam access not supported by this browser.";
            return;
        }
        
        // This call immediately prompts the user for permission.
        navigator.mediaDevices.getUserMedia({
            video: {
                width: { ideal: FRAME_WIDTH },
                height: { ideal: FRAME_HEIGHT }
            }
        })
            .then(stream => {
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play();
                    // Set canvas dimensions to match requested size
                    canvasOutput.width = FRAME_WIDTH;
                    canvasOutput.height = FRAME_HEIGHT;
                    isVideoReady = true;
                    
                    // Start cascade download only after camera is ready
                    initializeCascades(); 
                    
                    // Start the processing loop
                    window.requestAnimationFrame(processFrame);
                };
            })
            .catch(err => {
                statusElement.textContent = `Error accessing webcam: ${err.name} - ${err.message}. Permission was denied or camera is in use.`;
                console.error('Error accessing webcam:', err);
            });
    }

    // --- Core CV Functions ---

    function detectAndInitializeEyes(frame, frameGray, frameColor) {
        // All Mats and classifiers are guaranteed to be initialized here if areCascadesReady is true.
        const faces = new cv.RectVector();
        const eyes = new cv.RectVector();
        const minSize = new cv.Size(100, 100);

        // Detect faces (scaleFactor, minNeighbors match Python)
        faceCascade.detectMultiScale(frameGray, faces, 1.3, 7, 0, minSize, new cv.Size(0, 0));

        if (faces.size() === 0) {
            faces.delete();
            eyes.delete();
            return { success: false, points: null };
        }

        // Only process the largest face
        const faceRect = faces.get(0);
        
        // Draw the face bounding box 
        const point1 = new cv.Point(faceRect.x, faceRect.y);
        const point2 = new cv.Point(faceRect.x + faceRect.width, faceRect.y + faceRect.height);
        cv.rectangle(frameColor, point1, point2, [255, 165, 0, 255], 2);

        // Define ROI for eyes (upper half of the face)
        const roiRect = new cv.Rect(faceRect.x, faceRect.y, faceRect.width, faceRect.height / 2);
        const roiGray = frameGray.roi(roiRect);

        // Detect eyes within the ROI
        const eyeMinSize = new cv.Size(20, 20);
        eyeCascade.detectMultiScale(roiGray, eyes, 1.1, 8, 0, eyeMinSize, new cv.Size(0, 0));

        let success = false;
        let initialPoints = null;

        if (eyes.size() === 2) {
            const pointsArray = [];
            for (let i = 0; i < eyes.size(); ++i) {
                const eyeRect = eyes.get(i);
                
                // Calculate absolute center coordinates relative to the full frame
                const center_x = faceRect.x + eyeRect.x + eyeRect.width / 2;
                const center_y = faceRect.y + eyeRect.y + eyeRect.height / 2;
                
                pointsArray.push(center_x, center_y);

                // Draw eye bounding box (absolute coordinates)
                const eyeX = faceRect.x + eyeRect.x;
                const eyeY = faceRect.y + eyeRect.y;
                const eyeP1 = new cv.Point(eyeX, eyeY);
                const eyeP2 = new cv.Point(eyeX + eyeRect.width, eyeY + eyeRect.height);
                cv.rectangle(frameColor, eyeP1, eyeP2, [0, 255, 0, 255], 1);
            }
            
            // Create a Mat from the array of points (1 row, 2 columns, CV_32FC2 type)
            initialPoints = cv.matFromArray(pointsArray.length / 2, 1, cv.CV_32FC2, pointsArray);
            success = true;
        }

        // Cleanup
        roiGray.delete();
        faces.delete();
        eyes.delete();
        
        // Return points on success
        return { success: success, points: initialPoints }; 
    }

    function processFrame() {
        // Add safety check for the global object before using it, though it should be initialized.
        if (typeof cv === 'undefined' || !isVideoReady || !areCascadesReady) {
            window.requestAnimationFrame(processFrame);
            return;
        }

        // 1. Setup Mats
        const frame = cv.Mat.zeros(FRAME_HEIGHT, FRAME_WIDTH, cv.CV_8UC4);
        const frameGray = cv.Mat.zeros(FRAME_HEIGHT, FRAME_WIDTH, cv.CV_8UC1);

        // Draw the video frame onto a temporary canvas, then read it into the Mat (src)
        context.drawImage(video, 0, 0, FRAME_WIDTH, FRAME_HEIGHT);
        let src = cv.imread(canvasOutput);
        
        cv.cvtColor(src, frameGray, cv.COLOR_RGBA2GRAY);

        // --- State Logic and Blink Detection ---

        if (!trackingActive || old_points.rows < 2) {
            // --- Re-detection Phase ---
            
            const { success, points } = detectAndInitializeEyes(src, frameGray, src);

            if (success) {
                // Tracking restored/initialized
                old_points.delete(); 
                old_points = points.clone(); 
                points.delete(); // Cleanup Mat returned from detectAndInitializeEyes
                
                old_gray.delete();
                old_gray = frameGray.clone(); 
                
                // BLINK COUNTING LOGIC (Transitioning from lost state)
                if (!trackingActive) {
                    if (blinkClosedFrames >= BLINK_FRAME_THRESHOLD && blinkClosedFrames <= MAX_BLINK_FRAMES) {
                        blinkCounter++;
                        console.log(`Blink detected! Total Blinks: ${blinkCounter} (Closed Duration: ${blinkClosedFrames} frames)`);
                    } else if (blinkClosedFrames > 0) {
                        console.log(`Interruption detected (${blinkClosedFrames} frames) but NOT counted as blink. Reason: ${blinkClosedFrames < BLINK_FRAME_THRESHOLD ? 'Too short' : 'Too long'}.`);
                    }
                }

                trackingActive = true;
                blinkClosedFrames = 0;
                
            } else {
                // Detection failed (eyes closed/moved). Accumulate closed frames.
                blinkClosedFrames++;
            }

        } else {
            // --- Active Tracking Phase (Lucas-Kanade) ---

            const newPoints = new cv.Mat();
            const status = new cv.Mat();
            const err = new cv.Mat();
            
            const winSize = new cv.Size(21, 21);
            const maxLevel = 3;
            const criteria = new cv.TermCriteria(cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_COUNT, 20, 0.01);

            // 2. Lucas-Kanade calculation
            cv.calcOpticalFlowPyrLK(old_gray, frameGray, old_points, newPoints, status, err, winSize, maxLevel, criteria);

            let goodPointsCount = 0;
            const goodNewPointsData = [];

            // Filter successful points
            for (let i = 0; i < status.rows; ++i) {
                if (status.data[i] === 1) {
                    const x = newPoints.data32F[i * 2];
                    const y = newPoints.data32F[i * 2 + 1];
                    goodPointsCount++;
                    goodNewPointsData.push(x, y);

                    // 3. Drawing
                    const center = new cv.Point(Math.round(x), Math.round(y));
                    // Draw a filled green circle (TRACK_COLOR is [0, 255, 0, 255] in RGBA)
                    cv.circle(src, center, 7, [0, 255, 0, 255], -1, cv.LINE_AA);
                }
            }
            
            if (goodPointsCount === 2) {
                // Tracking successful
                old_points.delete(); 
                old_points = cv.matFromArray(goodNewPointsData.length / 2, 1, cv.CV_32FC2, goodNewPointsData);
                
                old_gray.delete();
                old_gray = frameGray.clone();

                blinkClosedFrames = 0;

            } else {
                // Tracking lost
                console.log("TRACKING LOST: Less than 2 points survived. Entering potential blink/re-detection phase.");
                trackingActive = false;
            }

            // Cleanup Mats used in LK
            newPoints.delete();
            status.delete();
            err.delete();
        }

        // Display status text at the bottom
        const statusMessage = trackingActive ? "STATUS: Tracking Eyes (Green Circles)" : "STATUS: Auto-detecting eyes...";
        cv.putText(src, statusMessage, new cv.Point(FRAME_WIDTH / 2 - 150, FRAME_HEIGHT - 20), 
                   cv.FONT_HERSHEY_SIMPLEX, 0.7, [255, 255, 255, 255], 2, cv.LINE_AA);

        // --- Final Display ---
        cv.imshow('canvasOutput', src);
        blinkCountElement.textContent = blinkCounter;
        statusElement.textContent = statusMessage.replace("STATUS: ", "");

        // Cleanup main Mats
        src.delete();
        frame.delete();
        frameGray.delete();

        // Loop: Request next frame
        window.requestAnimationFrame(processFrame);
    }

    // --- OpenCV Initialization Callback ---
    cv.onRuntimeInitialized = () => {
        // FIX: Initialize global Mat objects here, where 'cv' is guaranteed to be defined.
        old_gray = new cv.Mat();
        old_points = new cv.Mat();
        
        // 1. Immediately request camera access, which triggers the permission pop-up.
        startCamera();
    };
    
</script>
</body>
</html>
